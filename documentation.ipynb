{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook provides a detailed walkthrough of the pushshift_python analytics wrapper\n",
    "\n",
    "## https://github.com/casonk/pushshift_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all required packages/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zstandard\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create query superclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class query:\n",
    "    \"\"\"\n",
    "    SuperClass for compiling reddit queries.\n",
    "    ----------\n",
    "    paramaters\n",
    "    ----------\n",
    "    query_type:\n",
    "        subreddit- query provided subreddit.\n",
    "        keyword- query all subreddits for provided keyword.\n",
    "    query: provided subreddit or keyword.\n",
    "    time_range: dictionary input {'before' : latest post time, 'after' : earliest post time}\n",
    "        times can be given in unix epoch timestamp or datetime format.\n",
    "    time_format:\n",
    "        'unix'- defaults to unix epoch timestamp.\n",
    "        'datetime'- set this option is specifing time_range in datetime format.\n",
    "    post_type: selection to query for comments or submissions, defaults to both.\n",
    "        'comment'- only query comments.\n",
    "        'submission'- only query submission.\n",
    "        defaults to query both comments and submissions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, query_type, query, time_range, time_format=\"unix\", post_type=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initilization of query object.\n",
    "        \"\"\"\n",
    "        self.type = query_type.lower()\n",
    "        self.query = query.lower()\n",
    "        if time_format == \"datetime\":\n",
    "            time_range[\"before\"] = int(\n",
    "                datetime.datetime.timestamp(\n",
    "                    datetime.datetime.strptime(time_range[\"before\"], \"%Y-%m-%d\")\n",
    "                )\n",
    "            )\n",
    "            time_range[\"after\"] = int(\n",
    "                datetime.datetime.timestamp(\n",
    "                    datetime.datetime.strptime(time_range[\"after\"], \"%Y-%m-%d\")\n",
    "                )\n",
    "            )\n",
    "        self.before = int(time_range[\"before\"])\n",
    "        self.before_dt = datetime.datetime.fromtimestamp(self.before)\n",
    "        self.after = int(time_range[\"after\"])\n",
    "        self.after_dt = datetime.datetime.fromtimestamp(self.after)\n",
    "        try:\n",
    "            self.post_type = post_type.lower()\n",
    "        except:\n",
    "            self.post_type = post_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pushshift file query object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class pushshift_file_query(query):\n",
    "    \"\"\"\n",
    "    Class for compiling pushshift file queries.\n",
    "    Respective files can be downloaded from : https://files.pushshift.io/reddit/\n",
    "    ----------\n",
    "    paramaters\n",
    "    ----------\n",
    "    query_type:\n",
    "        'subreddit'- query provided subreddit.\n",
    "        'keyword'- query all subreddits for provided keyword.\n",
    "    query: provided subreddit or keyword.\n",
    "    time_range: dictionary input {'before' : latest post time, 'after' : earliest post time}\n",
    "        times can be given in unix epoch timestamp or datetime format.\n",
    "    time_format:\n",
    "        'unix'- defaults to unix epoch timestamp.\n",
    "        'datetime'- set this option is specifing time_range in datetime format.\n",
    "    post_type: selection to query for comments or submissions, defaults to both.\n",
    "        'comment'- only query comments.\n",
    "        'submission'- only query submission.\n",
    "        defaults to query both comments and submissions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, query_type, query, time_range, time_format=\"unix\", post_type=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initilization of query object.\n",
    "        \"\"\"\n",
    "        super().__init__(query_type, query, time_range, time_format, post_type)\n",
    "        self.submission_folder_path = Path(\n",
    "            \"F:/Research/Funded/Ethical_Reccomendations/Python/Push_File/Submissions/RS/2019+/\"\n",
    "        )\n",
    "        self.comment_folder_path = Path(\n",
    "            \"F:/Research/Funded/Ethical_Reccomendations/Python/Push_File/Comments/RC/2019+/\"\n",
    "        )\n",
    "        self.line_counter = 0\n",
    "        self.post_counter = 0\n",
    "        self.file_counter = 0\n",
    "        self.errors = 0\n",
    "\n",
    "    def set_parent_folders(self, submission_folder_path, comment_folder_path):\n",
    "        \"\"\"\n",
    "        Set paths to pushshift files.\n",
    "        \"\"\"\n",
    "        self.submission_folder_path = Path(submission_folder_path)\n",
    "        self.comment_folder_path = Path(comment_folder_path)\n",
    "\n",
    "    def read_lines_zst(self):\n",
    "        \"\"\"\n",
    "        Helper function for reading from ztandandard compressed ndjson files.\n",
    "        \"\"\"\n",
    "        with open(self.working_file, \"rb\") as file_handle:\n",
    "            buffer = \"\"\n",
    "            reader = zstandard.ZstdDecompressor(max_window_size=2 ** 31).stream_reader(\n",
    "                file_handle\n",
    "            )\n",
    "            while True:\n",
    "                chunk = reader.read(2 ** 27).decode()\n",
    "                if not chunk:\n",
    "                    break\n",
    "                lines = (buffer + chunk).split(\"\\n\")\n",
    "\n",
    "                for line in lines[:-1]:\n",
    "                    yield line, file_handle.tell()\n",
    "\n",
    "                buffer = lines[-1]\n",
    "            reader.close()\n",
    "\n",
    "    def make_query(self):\n",
    "        \"\"\"\n",
    "        Initialize the query.\n",
    "        \"\"\"\n",
    "        self.df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"post_type\",\n",
    "                \"subreddit\",\n",
    "                \"id\",\n",
    "                \"parent_id\",\n",
    "                \"link_id\",\n",
    "                \"url\",\n",
    "                \"permalink\",\n",
    "                \"created_utc\",\n",
    "                \"datetime\",\n",
    "                \"score\",\n",
    "                \"num_comments\",\n",
    "                \"title\",\n",
    "                \"body\",\n",
    "                \"author\",\n",
    "            ]\n",
    "        )\n",
    "        self.submissions = self.df.copy()\n",
    "        self.comments = self.df.copy()\n",
    "\n",
    "        def create_common_data(post):\n",
    "            \"\"\"\n",
    "            Helper function to collect values common between both comments and submissions.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                subreddit = post[\"subreddit\"]\n",
    "                post_id = post[\"id\"]\n",
    "                try:\n",
    "                    parent_id = post[\"parent_id\"]\n",
    "                except KeyError:\n",
    "                    parent_id = \"nan\"\n",
    "                try:\n",
    "                    link_id = post[\"link_id\"]\n",
    "                except KeyError:\n",
    "                    link_id = \"nan\"\n",
    "                try:\n",
    "                    url = post[\"url\"]\n",
    "                except KeyError:\n",
    "                    url = \"nan\"\n",
    "                permalink = post[\"permalink\"]\n",
    "                created_utc = post[\"created_utc\"]\n",
    "                t = datetime.datetime.fromtimestamp(created_utc)\n",
    "                date = t.strftime(\"%m/%d/%Y\")\n",
    "                score = post[\"score\"]\n",
    "                try:\n",
    "                    num_comments = post[\"num_comments\"]\n",
    "                except KeyError:\n",
    "                    num_comments = \"nan\"\n",
    "                try:\n",
    "                    title = post[\"title\"]\n",
    "                    title = r\"{}\".format(title)\n",
    "                except KeyError:\n",
    "                    title = \"nan\"\n",
    "                author = post[\"author\"]\n",
    "                author = r\"{}\".format(author)\n",
    "                return (\n",
    "                    subreddit,\n",
    "                    post_id,\n",
    "                    parent_id,\n",
    "                    link_id,\n",
    "                    url,\n",
    "                    permalink,\n",
    "                    date,\n",
    "                    created_utc,\n",
    "                    score,\n",
    "                    num_comments,\n",
    "                    title,\n",
    "                    author,\n",
    "                )\n",
    "            except KeyboardInterrupt:\n",
    "                pass\n",
    "\n",
    "        def search_sumissions(self):\n",
    "            for line, file_bytes_processed in self.read_lines_zst():\n",
    "                self.line_counter += 1\n",
    "                if self.line_counter % 1000000 == 0:\n",
    "                    print(\n",
    "                        \"  >> Processed {} Posts, Found {} Posts\".format(\n",
    "                            self.line_counter, self.post_counter\n",
    "                        )\n",
    "                    )\n",
    "                try:\n",
    "                    post = json.loads(line)\n",
    "                    if self.type == \"subreddit\":\n",
    "                        if int(post[\"created_utc\"]) >= int(self.after):\n",
    "                            if int(post[\"created_utc\"]) <= int(self.before):\n",
    "                                if post[\"subreddit\"] == self.query:\n",
    "                                    self.post_counter += 1\n",
    "                                    (\n",
    "                                        subreddit,\n",
    "                                        post_id,\n",
    "                                        parent_id,\n",
    "                                        link_id,\n",
    "                                        url,\n",
    "                                        permalink,\n",
    "                                        date,\n",
    "                                        created_utc,\n",
    "                                        score,\n",
    "                                        num_comments,\n",
    "                                        title,\n",
    "                                        author,\n",
    "                                    ) = create_common_data(post=post)\n",
    "                                    try:\n",
    "                                        body = post[\"selftext\"]\n",
    "                                        body = r\"{}\".format(body)\n",
    "                                    except KeyError:\n",
    "                                        body = \"nan\"\n",
    "                                    post_data = {\n",
    "                                        \"post_type\": \"submission\",\n",
    "                                        \"subreddit\": subreddit,\n",
    "                                        \"id\": post_id,\n",
    "                                        \"parent_id\": parent_id,\n",
    "                                        \"link_id\": link_id,\n",
    "                                        \"url\": url,\n",
    "                                        \"permalink\": permalink,\n",
    "                                        \"created_utc\": created_utc,\n",
    "                                        \"datetime\": date,\n",
    "                                        \"score\": score,\n",
    "                                        \"num_comments\": num_comments,\n",
    "                                        \"title\": title,\n",
    "                                        \"body\": body,\n",
    "                                        \"author\": author,\n",
    "                                    }\n",
    "                                    try:\n",
    "                                        self.submissions = self.submissions.append(\n",
    "                                            post_data, ignore_index=True\n",
    "                                        )\n",
    "                                    except KeyboardInterrupt:\n",
    "                                        self.submissions = self.submissions.append(\n",
    "                                            post_data, ignore_index=True\n",
    "                                        )\n",
    "                                        print(\n",
    "                                            \"Keyboard Interrupt Detected, please Interrupt again to break parent function.\"\n",
    "                                        )\n",
    "                                        break\n",
    "                            # elif self.query_type == 'keyword':\n",
    "                except (KeyError, json.JSONDecodeError):\n",
    "                    self.errors += 1\n",
    "\n",
    "        def search_comments(self):\n",
    "            for line, file_bytes_processed in self.read_lines_zst():\n",
    "                self.line_counter += 1\n",
    "                if self.line_counter % 1000000 == 0:\n",
    "                    print(\n",
    "                        \"  >> Processed {} Posts, Found {} Posts\".format(\n",
    "                            self.line_counter, self.post_counter\n",
    "                        )\n",
    "                    )\n",
    "                try:\n",
    "                    post = json.loads(line)\n",
    "                    if self.type == \"subreddit\":\n",
    "                        if int(post[\"created_utc\"]) >= int(self.after):\n",
    "                            if int(post[\"created_utc\"]) <= int(self.before):\n",
    "                                if post[\"subreddit\"] == self.query:\n",
    "                                    self.post_counter += 1\n",
    "                                    (\n",
    "                                        subreddit,\n",
    "                                        post_id,\n",
    "                                        parent_id,\n",
    "                                        link_id,\n",
    "                                        url,\n",
    "                                        permalink,\n",
    "                                        date,\n",
    "                                        created_utc,\n",
    "                                        score,\n",
    "                                        num_comments,\n",
    "                                        title,\n",
    "                                        author,\n",
    "                                    ) = create_common_data(post=post)\n",
    "                                    try:\n",
    "                                        body = post[\"body\"]\n",
    "                                        body = r\"{}\".format(body)\n",
    "                                    except KeyError:\n",
    "                                        body = \"nan\"\n",
    "                                    post_data = {\n",
    "                                        \"post_type\": \"comment\",\n",
    "                                        \"subreddit\": subreddit,\n",
    "                                        \"id\": post_id,\n",
    "                                        \"parent_id\": parent_id,\n",
    "                                        \"link_id\": link_id,\n",
    "                                        \"url\": url,\n",
    "                                        \"permalink\": permalink,\n",
    "                                        \"created_utc\": created_utc,\n",
    "                                        \"datetime\": date,\n",
    "                                        \"score\": score,\n",
    "                                        \"num_comments\": num_comments,\n",
    "                                        \"title\": title,\n",
    "                                        \"body\": body,\n",
    "                                        \"author\": author,\n",
    "                                    }\n",
    "                                    try:\n",
    "                                        self.comments = self.comments.append(\n",
    "                                            post_data, ignore_index=True\n",
    "                                        )\n",
    "                                    except KeyboardInterrupt:\n",
    "                                        self.comments = self.comments.append(\n",
    "                                            post_data, ignore_index=True\n",
    "                                        )\n",
    "                                        print(\n",
    "                                            \"Keyboard Interrupt Detected, please Interrupt again to break parent function.\"\n",
    "                                        )\n",
    "                                        break\n",
    "                            # elif self.query_type == 'keyword':\n",
    "                except (KeyError, json.JSONDecodeError):\n",
    "                    self.errors += 1\n",
    "\n",
    "        def make_time_list(self):\n",
    "            first = self.after_dt\n",
    "            last = self.before_dt\n",
    "            while first <= last:\n",
    "                self.time_list.append(first.strftime(\"%Y-%m\"))\n",
    "                first += relativedelta(months=1)\n",
    "            if last.strftime(\"%Y-%m\") in self.time_list:\n",
    "                pass\n",
    "            else:\n",
    "                self.time_list.append(last.strftime(\"%Y-%m\"))\n",
    "\n",
    "        make_time_list(self=self)\n",
    "\n",
    "        all_submission_files = [\n",
    "            submission_file for submission_file in self.submission_folder_path.iterdir()\n",
    "        ]\n",
    "        if self.post_type == \"comment\":\n",
    "            pass\n",
    "        else:\n",
    "            for file in all_submission_files:\n",
    "                try:\n",
    "                    for time in self.time_list:\n",
    "                        if time in file.name:\n",
    "                            self.working_file = str(file.as_posix())\n",
    "                            print(\"> Parsing : {}\".format(file.name))\n",
    "                            try:\n",
    "                                search_sumissions(self=self)\n",
    "                            except KeyboardInterrupt:\n",
    "                                print(\n",
    "                                    \"Keyboard Interrupt Detected, your object's values are secure\"\n",
    "                                )\n",
    "                                break\n",
    "                            self.file_counter += 1\n",
    "                            print(\n",
    "                                \"   >>> Total Files Parsed : {}, Total Posts Parsed : {}, Total Posts Collected : {}, Total Errors Found : {}\".format(\n",
    "                                    self.file_counter,\n",
    "                                    self.line_counter,\n",
    "                                    self.post_counter,\n",
    "                                    self.errors,\n",
    "                                )\n",
    "                            )\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\n",
    "                        \"Keyboard Interrupt Detected, your object's values are secure\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        all_comment_files = [\n",
    "            comment_file for comment_file in self.comment_folder_path.iterdir()\n",
    "        ]\n",
    "        if self.post_type == \"submission\":\n",
    "            pass\n",
    "        else:\n",
    "            for file in all_comment_files:\n",
    "                try:\n",
    "                    for time in self.time_list:\n",
    "                        if time in file.name:\n",
    "                            self.working_file = str(file.as_posix())\n",
    "                            print(\"> Parsing : {}\".format(file.name))\n",
    "                            try:\n",
    "                                search_comments(self=self)\n",
    "                            except KeyboardInterrupt:\n",
    "                                print(\n",
    "                                    \"Keyboard Interrupt Detected, your object's values are secure\"\n",
    "                                )\n",
    "                                break\n",
    "                            self.file_counter += 1\n",
    "                            print(\n",
    "                                \"   >>> Total Files Parsed : {}, Total Posts Parsed : {}, Total Posts Collected : {}, Total Errors Found : {}\".format(\n",
    "                                    self.file_counter,\n",
    "                                    self.line_counter,\n",
    "                                    self.post_counter,\n",
    "                                    self.errors,\n",
    "                                )\n",
    "                            )\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\n",
    "                        \"Keyboard Interrupt Detected, your object's values are secure\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        self.df = self.submissions.append(self.comments)\n",
    "\n",
    "    def export(self, path, to_export=\"df\", export_format=\".pkl\"):\n",
    "        \"\"\"\n",
    "        Easily save and export your data for future analytics.\n",
    "        ----------\n",
    "        paramaters\n",
    "        ----------\n",
    "        path: path to save output data to.\n",
    "        to_export: select what data you wish to export\n",
    "            'df'- all data.\n",
    "            'submissions'- only submission data.\n",
    "            'comments'- only comment data.\n",
    "        export_format:\n",
    "            'pkl'- default, exports to pickle.\n",
    "            'csv'- export to comma seperated file.\n",
    "        \"\"\"\n",
    "        if to_export == \"df\":\n",
    "            if export_format == \"pkl\":\n",
    "                self.df.to_pickle(path=path)\n",
    "            elif export_format == \"csv\":\n",
    "                self.df.to_csv(path_or_buf=path)\n",
    "        elif to_export == \"submissions\":\n",
    "            if export_format == \"pkl\":\n",
    "                self.submissions.to_pickle(path=path)\n",
    "            elif export_format == \"csv\":\n",
    "                self.submissions.to_csv(path_or_buf=path)\n",
    "        elif to_export == \"comments\":\n",
    "            if export_format == \"pkl\":\n",
    "                self.comments.to_pickle(path=path)\n",
    "            elif export_format == \"csv\":\n",
    "                self.comments.to_csv(path_or_buf=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example file query using datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "antivax_query = pushshift_file_query(\n",
    "    query_type=\"subreddit\",\n",
    "    query=\"antivax\",\n",
    "    time_range={\"before\": \"2019-06-26\", \"after\": \"2019-06-01\"},\n",
    "    time_format=\"datetime\",\n",
    "    post_type=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the file query\n",
    "\n",
    "Note this will take substantial time to sort though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antivax_query.make_query()\n",
    "antivax_query.time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pushshift web query object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class pushshift_web_query(query):\n",
    "    \"\"\"\n",
    "    Class for compiling pushshift web queries.\n",
    "    ----------\n",
    "    paramaters\n",
    "    ----------\n",
    "    query_type:\n",
    "        subreddit- query provided subreddit.\n",
    "        keyword- query all subreddits for provided keyword.\n",
    "    query: provided subreddit or keyword.\n",
    "    time_range: dictionary input {'before' : latest post time, 'after' : earliest post time}\n",
    "        times can be given in unix epoch timestamp or datetime format.\n",
    "    time_format:\n",
    "        'unix'- defaults to unix epoch timestamp.\n",
    "        'datetime'- set this option is specifing time_range in datetime format.\n",
    "    post_type: selection to query for comments or submissions, defaults to both.\n",
    "        'comment'- only query comments.\n",
    "        'submission'- only query submission.\n",
    "        defaults to query both comments and submissions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, query_type, query, time_range, time_format=\"unix\", post_type=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initilization of query object.\n",
    "        \"\"\"\n",
    "        super().__init__(query_type, query, time_range, time_format, post_type)\n",
    "        self.api_hit_counter = 0\n",
    "\n",
    "    def update_url(self):\n",
    "        \"\"\"\n",
    "        Helper function to update timestamp after each API request.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.type == \"subreddit\":\n",
    "                self.comment_url = \"https://api.pushshift.io/reddit/search/{}/?after={}&before={}&subreddit={}&size={}\".format(\n",
    "                    str(\"comment\"),\n",
    "                    str(self.current_time),\n",
    "                    str(self.before),\n",
    "                    str(self.query),\n",
    "                    \"12345\",\n",
    "                )\n",
    "                self.submission_url = \"https://api.pushshift.io/reddit/search/{}/?after={}&before={}&subreddit={}&size={}\".format(\n",
    "                    str(\"submission\"),\n",
    "                    str(self.current_time),\n",
    "                    str(self.before),\n",
    "                    str(self.query),\n",
    "                    \"12345\",\n",
    "                )\n",
    "            elif self.type == \"keyword\":\n",
    "                self.comment_url = \"https://api.pushshift.io/reddit/search/{}/?q={}&after={}&before={}&size={}\".format(\n",
    "                    str(\"comment\"),\n",
    "                    str(self.query),\n",
    "                    str(self.current_time),\n",
    "                    str(self.before),\n",
    "                    \"12345\",\n",
    "                )\n",
    "                self.submission_url = \"https://api.pushshift.io/reddit/search/{}/?q={}&after={}&before={}&size={}\".format(\n",
    "                    str(\"submission\"),\n",
    "                    str(self.query),\n",
    "                    str(self.current_time),\n",
    "                    str(self.before),\n",
    "                    \"12345\",\n",
    "                )\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "    def make_query(self):\n",
    "        \"\"\"\n",
    "        Initialize the query.\n",
    "        \"\"\"\n",
    "        self.df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"post_type\",\n",
    "                \"subreddit\",\n",
    "                \"id\",\n",
    "                \"parent_id\",\n",
    "                \"link_id\",\n",
    "                \"url\",\n",
    "                \"permalink\",\n",
    "                \"created_utc\",\n",
    "                \"datetime\",\n",
    "                \"score\",\n",
    "                \"num_comments\",\n",
    "                \"title\",\n",
    "                \"body\",\n",
    "                \"author\",\n",
    "            ]\n",
    "        )\n",
    "        self.submissions = self.df.copy()\n",
    "        self.comments = self.df.copy()\n",
    "\n",
    "        def web_hit(self, url):\n",
    "            \"\"\"\n",
    "            Helper function to make the API request.\n",
    "            ----------\n",
    "            paramaters\n",
    "            ----------\n",
    "            url: provide either self.submission_url or self.comment_url depending on post type.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                self.api_hit_counter += 1\n",
    "                try:\n",
    "                    r = requests.get(url)\n",
    "                    status = r.status_code\n",
    "                    print(\"> http response is:\", status)\n",
    "                except:\n",
    "                    status = \"NO HANDSHAKE WITH API\"\n",
    "                    print(status)\n",
    "                if status != 200:\n",
    "                    retry = 0\n",
    "                    while True:\n",
    "                        retry += 1\n",
    "                        print(\n",
    "                            \"\\nAPI DECLINED REQUEST\\n\\n>> This is retry #:\",\n",
    "                            retry,\n",
    "                            \"<<\\n\",\n",
    "                        )\n",
    "                        time.sleep(15 * retry)\n",
    "                        try:\n",
    "                            r = requests.get(url)\n",
    "                            status = r.status_code\n",
    "                            print(\">> retry http response is:\", status)\n",
    "                        except:\n",
    "                            status = \"NO HANDSHAKE WITH API\"\n",
    "                            print(status)\n",
    "                        if status == 200:\n",
    "                            break\n",
    "                print(\" >> Web Hit On\", self.query, \"# :\", self.api_hit_counter)\n",
    "                print(\n",
    "                    \"  >>> Current Post Time :\",\n",
    "                    str(datetime.datetime.fromtimestamp(self.current_time)),\n",
    "                )\n",
    "                self.web_data = json.loads(r.text, strict=False)\n",
    "                time.sleep(1)\n",
    "            except KeyboardInterrupt:\n",
    "                pass\n",
    "\n",
    "        def create_common_data(post):\n",
    "            \"\"\"\n",
    "            Helper function to collect values common between both comments and submissions.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                subreddit = post[\"subreddit\"]\n",
    "                post_id = post[\"id\"]\n",
    "                try:\n",
    "                    parent_id = post[\"parent_id\"]\n",
    "                except KeyError:\n",
    "                    parent_id = \"nan\"\n",
    "                try:\n",
    "                    link_id = post[\"link_id\"]\n",
    "                except KeyError:\n",
    "                    link_id = \"nan\"\n",
    "                try:\n",
    "                    url = post[\"url\"]\n",
    "                except KeyError:\n",
    "                    url = \"nan\"\n",
    "                permalink = post[\"permalink\"]\n",
    "                created_utc = post[\"created_utc\"]\n",
    "                t = datetime.datetime.fromtimestamp(created_utc)\n",
    "                date = t.strftime(\"%m/%d/%Y\")\n",
    "                score = post[\"score\"]\n",
    "                try:\n",
    "                    num_comments = post[\"num_comments\"]\n",
    "                except KeyError:\n",
    "                    num_comments = \"nan\"\n",
    "                try:\n",
    "                    title = post[\"title\"]\n",
    "                    title = r\"{}\".format(title)\n",
    "                except KeyError:\n",
    "                    title = \"nan\"\n",
    "                author = post[\"author\"]\n",
    "                author = r\"{}\".format(author)\n",
    "                return (\n",
    "                    subreddit,\n",
    "                    post_id,\n",
    "                    parent_id,\n",
    "                    link_id,\n",
    "                    url,\n",
    "                    permalink,\n",
    "                    date,\n",
    "                    created_utc,\n",
    "                    score,\n",
    "                    num_comments,\n",
    "                    title,\n",
    "                    author,\n",
    "                )\n",
    "            except KeyboardInterrupt:\n",
    "                pass\n",
    "\n",
    "        def save_submissions(self):\n",
    "            \"\"\"\n",
    "            Helper function to save submissions to self.submissions.\n",
    "            \"\"\"\n",
    "            for post in self.web_data[\"data\"]:\n",
    "                (\n",
    "                    subreddit,\n",
    "                    post_id,\n",
    "                    parent_id,\n",
    "                    link_id,\n",
    "                    url,\n",
    "                    permalink,\n",
    "                    date,\n",
    "                    created_utc,\n",
    "                    score,\n",
    "                    num_comments,\n",
    "                    title,\n",
    "                    author,\n",
    "                ) = create_common_data(post=post)\n",
    "                try:\n",
    "                    body = post[\"selftext\"]\n",
    "                    body = r\"{}\".format(body)\n",
    "                except KeyError:\n",
    "                    body = \"nan\"\n",
    "                post_data = {\n",
    "                    \"post_type\": \"submission\",\n",
    "                    \"subreddit\": subreddit,\n",
    "                    \"id\": post_id,\n",
    "                    \"parent_id\": parent_id,\n",
    "                    \"link_id\": link_id,\n",
    "                    \"url\": url,\n",
    "                    \"permalink\": permalink,\n",
    "                    \"created_utc\": created_utc,\n",
    "                    \"datetime\": date,\n",
    "                    \"score\": score,\n",
    "                    \"num_comments\": num_comments,\n",
    "                    \"title\": title,\n",
    "                    \"body\": body,\n",
    "                    \"author\": author,\n",
    "                }\n",
    "                try:\n",
    "                    self.submissions = self.submissions.append(\n",
    "                        post_data, ignore_index=True\n",
    "                    )\n",
    "                    self.current_time = created_utc\n",
    "                except KeyboardInterrupt:\n",
    "                    self.submissions = self.submissions.append(\n",
    "                        post_data, ignore_index=True\n",
    "                    )\n",
    "                    self.current_time = created_utc\n",
    "                    print(\n",
    "                        \"Keyboard Interrupt Detected, please Interrupt again to break parent function.\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        def save_comments(self):\n",
    "            \"\"\"\n",
    "            Helper function to save comments to self.comments.\n",
    "            \"\"\"\n",
    "            for post in self.web_data[\"data\"]:\n",
    "                (\n",
    "                    subreddit,\n",
    "                    post_id,\n",
    "                    parent_id,\n",
    "                    link_id,\n",
    "                    url,\n",
    "                    permalink,\n",
    "                    date,\n",
    "                    created_utc,\n",
    "                    score,\n",
    "                    num_comments,\n",
    "                    title,\n",
    "                    author,\n",
    "                ) = create_common_data(post=post)\n",
    "                try:\n",
    "                    body = post[\"body\"]\n",
    "                    body = r\"{}\".format(body)\n",
    "                except KeyError:\n",
    "                    body = \"nan\"\n",
    "                post_data = {\n",
    "                    \"post_type\": \"comment\",\n",
    "                    \"subreddit\": subreddit,\n",
    "                    \"id\": post_id,\n",
    "                    \"parent_id\": parent_id,\n",
    "                    \"link_id\": link_id,\n",
    "                    \"url\": url,\n",
    "                    \"permalink\": permalink,\n",
    "                    \"created_utc\": created_utc,\n",
    "                    \"datetime\": date,\n",
    "                    \"score\": score,\n",
    "                    \"num_comments\": num_comments,\n",
    "                    \"title\": title,\n",
    "                    \"body\": body,\n",
    "                    \"author\": author,\n",
    "                }\n",
    "                try:\n",
    "                    self.comments = self.comments.append(post_data, ignore_index=True)\n",
    "                    self.current_time = created_utc\n",
    "                except KeyboardInterrupt:\n",
    "                    self.submissions = self.submissions.append(\n",
    "                        post_data, ignore_index=True\n",
    "                    )\n",
    "                    self.current_time = created_utc\n",
    "                    print(\n",
    "                        \"Keyboard Interrupt Detected, please Interrupt again to break parent function.\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        def collect_submissions(self):\n",
    "            \"\"\"\n",
    "            Master function to chain previous helper functions and collect the requested data for submissions.\n",
    "            \"\"\"\n",
    "            self.current_time = self.after\n",
    "            if self.post_type == \"comment\":\n",
    "                pass\n",
    "            else:\n",
    "                while self.current_time < self.before:\n",
    "                    self.update_url()\n",
    "                    web_hit(self=self, url=self.submission_url)\n",
    "                    # print(self.web_data)\n",
    "                    if len(self.web_data[\"data\"]) == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        try:\n",
    "                            save_submissions(self=self)\n",
    "                        except KeyboardInterrupt:\n",
    "                            print(\n",
    "                                \"Keyboard Interrupt Detected, your object's values are secure\"\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "        def collect_comments(self):\n",
    "            \"\"\"\n",
    "            Master function to chain previous helper functions and collect the requested data for comments.\n",
    "            \"\"\"\n",
    "            self.current_time = self.after\n",
    "            if self.post_type == \"submission\":\n",
    "                pass\n",
    "            else:\n",
    "                while self.current_time < self.before:\n",
    "                    self.update_url()\n",
    "                    web_hit(self=self, url=self.comment_url)\n",
    "                    if len(self.web_data[\"data\"]) == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        try:\n",
    "                            save_comments(self=self)\n",
    "                        except KeyboardInterrupt:\n",
    "                            print(\n",
    "                                \"Keyboard Interrupt Detected, your object's values are secure\"\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "        collect_submissions(self=self)\n",
    "        collect_comments(self=self)\n",
    "\n",
    "        self.df = self.submissions.append(self.comments)\n",
    "\n",
    "    def export(self, path, to_export=\"df\", export_format=\"pkl\"):\n",
    "        \"\"\"\n",
    "        Easily save and export your data for future analytics.\n",
    "        ----------\n",
    "        paramaters\n",
    "        ----------\n",
    "        path: path to save output data to.\n",
    "        to_export: select what data you wish to export\n",
    "            'df'- all data.\n",
    "            'submissions'- only submission data.\n",
    "            'comments'- only comment data.\n",
    "        export_format:\n",
    "            'pkl'- default, exports to pickle.\n",
    "            'csv'- export to comma seperated file.\n",
    "        \"\"\"\n",
    "        if to_export == \"df\":\n",
    "            if export_format == \"pkl\":\n",
    "                self.df.to_pickle(path=path)\n",
    "            elif export_format == \"csv\":\n",
    "                self.df.to_csv(path_or_buf=path)\n",
    "        elif to_export == \"submissions\":\n",
    "            if export_format == \"pkl\":\n",
    "                self.submissions.to_pickle(path=path)\n",
    "            elif export_format == \"csv\":\n",
    "                self.submissions.to_csv(path_or_buf=path)\n",
    "        elif to_export == \"comments\":\n",
    "            if export_format == \"pkl\":\n",
    "                self.comments.to_pickle(path=path)\n",
    "            elif export_format == \"csv\":\n",
    "                self.comments.to_csv(path_or_buf=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PushShift Web Query Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PushShift Web Query using unix epoch time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "conspiracy_query = pushshift_web_query(\n",
    "    query_type=\"subreddit\",\n",
    "    query=\"conspiracy\",\n",
    "    time_range={\"before\": \"1609631999\", \"after\": \"1609462861\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PushShift Web Query using datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "antivax_query = pushshift_web_query(\n",
    "    query_type=\"subreddit\",\n",
    "    query=\"antivax\",\n",
    "    time_range={\"before\": \"2021-01-02\", \"after\": \"2021-01-01\"},\n",
    "    time_format=\"datetime\",\n",
    "    post_type=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the query request\n",
    "\n",
    "Note, this will take some time to collect the API request, timing will depend on specified time_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conspiracy_query.make_query()\n",
    "antivax_query.make_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our queries return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_type</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>datetime</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>submission</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ko20vl</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.reddit.com/r/conspiracy/comments/k...</td>\n",
       "      <td>/r/conspiracy/comments/ko20vl/i_want_to_know_w...</td>\n",
       "      <td>1609462898</td>\n",
       "      <td>12/31/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>I want to know what’s going on</td>\n",
       "      <td>I’ve listened to all 3 Joe Rohan and Alex Jone...</td>\n",
       "      <td>Bmille3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>submission</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ko21bc</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.reddit.com/r/conspiracy/comments/k...</td>\n",
       "      <td>/r/conspiracy/comments/ko21bc/why_the_world_ha...</td>\n",
       "      <td>1609462943</td>\n",
       "      <td>12/31/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>Why the world has not ended yet?</td>\n",
       "      <td>So many people are aware of 666, I don’t under...</td>\n",
       "      <td>yotta_e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>submission</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ko22uq</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://i.redd.it/ub4tr6ykem861.jpg</td>\n",
       "      <td>/r/conspiracy/comments/ko22uq/mf_dooms_last_po...</td>\n",
       "      <td>1609463098</td>\n",
       "      <td>12/31/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>MF Dooms last post on IG, #33 #freemasonryobse...</td>\n",
       "      <td></td>\n",
       "      <td>Vegannibba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_type   subreddit      id parent_id link_id  \\\n",
       "0  submission  conspiracy  ko20vl       nan     nan   \n",
       "1  submission  conspiracy  ko21bc       nan     nan   \n",
       "2  submission  conspiracy  ko22uq       nan     nan   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/conspiracy/comments/k...   \n",
       "1  https://www.reddit.com/r/conspiracy/comments/k...   \n",
       "2                https://i.redd.it/ub4tr6ykem861.jpg   \n",
       "\n",
       "                                           permalink created_utc    datetime  \\\n",
       "0  /r/conspiracy/comments/ko20vl/i_want_to_know_w...  1609462898  12/31/2020   \n",
       "1  /r/conspiracy/comments/ko21bc/why_the_world_ha...  1609462943  12/31/2020   \n",
       "2  /r/conspiracy/comments/ko22uq/mf_dooms_last_po...  1609463098  12/31/2020   \n",
       "\n",
       "  score num_comments                                              title  \\\n",
       "0     1           39                     I want to know what’s going on   \n",
       "1     1           37                   Why the world has not ended yet?   \n",
       "2     1           14  MF Dooms last post on IG, #33 #freemasonryobse...   \n",
       "\n",
       "                                                body      author  \n",
       "0  I’ve listened to all 3 Joe Rohan and Alex Jone...     Bmille3  \n",
       "1  So many people are aware of 666, I don’t under...     yotta_e  \n",
       "2                                                     Vegannibba  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conspiracy_query.df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_type</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>datetime</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>antivax</td>\n",
       "      <td>ghohvb4</td>\n",
       "      <td>t3_knu1v8</td>\n",
       "      <td>t3_knu1v8</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/antivax/comments/knu1v8/you_too_are_excited...</td>\n",
       "      <td>1609479929</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>What</td>\n",
       "      <td>ajcabelera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>antivax</td>\n",
       "      <td>ghokmmf</td>\n",
       "      <td>t3_knpv4r</td>\n",
       "      <td>t3_knpv4r</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/antivax/comments/knpv4r/mmr_vs_measles/ghok...</td>\n",
       "      <td>1609482195</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Why not use \"dead\" vaccine to reduce risk? Why...</td>\n",
       "      <td>MichaelAChristian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>antivax</td>\n",
       "      <td>ghomglv</td>\n",
       "      <td>t3_knpv4r</td>\n",
       "      <td>t3_knpv4r</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/antivax/comments/knpv4r/mmr_vs_measles/ghom...</td>\n",
       "      <td>1609483787</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>&amp;gt;Here are the serious side effects of MMR \\...</td>\n",
       "      <td>slip63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_type subreddit       id  parent_id    link_id  url  \\\n",
       "0   comment   antivax  ghohvb4  t3_knu1v8  t3_knu1v8  nan   \n",
       "1   comment   antivax  ghokmmf  t3_knpv4r  t3_knpv4r  nan   \n",
       "2   comment   antivax  ghomglv  t3_knpv4r  t3_knpv4r  nan   \n",
       "\n",
       "                                           permalink created_utc    datetime  \\\n",
       "0  /r/antivax/comments/knu1v8/you_too_are_excited...  1609479929  01/01/2021   \n",
       "1  /r/antivax/comments/knpv4r/mmr_vs_measles/ghok...  1609482195  01/01/2021   \n",
       "2  /r/antivax/comments/knpv4r/mmr_vs_measles/ghom...  1609483787  01/01/2021   \n",
       "\n",
       "  score num_comments title                                               body  \\\n",
       "0     1          nan   nan                                               What   \n",
       "1     1          nan   nan  Why not use \"dead\" vaccine to reduce risk? Why...   \n",
       "2     1          nan   nan  &gt;Here are the serious side effects of MMR \\...   \n",
       "\n",
       "              author  \n",
       "0         ajcabelera  \n",
       "1  MichaelAChristian  \n",
       "2             slip63  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antivax_query.comments.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create community object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class community:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=\"community\",\n",
    "        path=None,\n",
    "        dataframe=None,\n",
    "        columns=None,\n",
    "        file_format=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initilization of object, created DataFrame for provided community.\n",
    "        ----------\n",
    "        paramaters\n",
    "        ----------\n",
    "        path: path to file location.\n",
    "        datafram: pass a corresponding community dataframe.\n",
    "        columns: selected colums to read, only applicable for .csv.\n",
    "        file_format: defaults to \"None\" when passed a pandas dataframe.\n",
    "            \"csv\"- for passing DataFrame stored as csv.\n",
    "            \"pkl\"- for passing a pickled DataFrame.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        if path == None:\n",
    "            self.df = dataframe\n",
    "        elif file_format == \"csv\":\n",
    "            self.df = pd.read_csv(\n",
    "                filepath_or_buffer=path, usecols=columns, low_memory=False\n",
    "            )\n",
    "        elif file_format == \"pkl\":\n",
    "            self.df = pd.read_pickle(filepath_or_buffer=path)\n",
    "        submission_mask = self.df[\"post_type\"] == \"submission\"\n",
    "        comment_mask = self.df[\"post_type\"] == \"comment\"\n",
    "        self.submissions = self.df[submission_mask]\n",
    "        self.comments = self.df[comment_mask]\n",
    "\n",
    "    def make_urls(self, column=\"body\", post_type=None):\n",
    "        \"\"\"\n",
    "        Qurey posts for url embeddings in posts.\n",
    "        ----------\n",
    "        paramaters\n",
    "        ----------\n",
    "        column: DataFrame column to query on, body or title.\n",
    "        post_type: option to restrict posts to only comments or submissions.\n",
    "        Note* submissions usuallly contain a seperate embedded url field.\n",
    "        \"\"\"\n",
    "        url_pattern = re.compile(\"((www\\.[^\\s]+)|(https://[^\\s]+))\")\n",
    "\n",
    "        def find_urls(frame):\n",
    "            mask = frame[column].str.match(url_pattern, na=False)\n",
    "            self.url_df = frame[mask]\n",
    "            self.urls = pd.DataFrame(\n",
    "                self.url_df[column].str.extract(url_pattern)[0].rename(\"url\")\n",
    "            )\n",
    "        if post_type == None:\n",
    "            find_urls(frame=self.df)\n",
    "        elif post_type == \"comment\":\n",
    "            comment_mask = self.df[\"post_type\"] == \"comment\"\n",
    "            find_urls(frame=self.df[comment_mask])\n",
    "        elif post_type == \"submission\":\n",
    "            submission_mask = self.df[\"post_type\"] == \"submission\"\n",
    "            find_urls(frame=self.df[submission_mask])\n",
    "\n",
    "    def make_authors(self):\n",
    "        indx = self.df[\"author\"].unique()\n",
    "        self.authors = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"total_submissions\",\n",
    "                \"total_submission_score\",\n",
    "                \"total_submission_comments\",\n",
    "                \"total_comments\",\n",
    "                \"total_comment_score\",\n",
    "                \"total_posts\",\n",
    "                \"total_post_score\",\n",
    "            ],\n",
    "            index=indx,\n",
    "        )\n",
    "        type_mask = self.df[\"post_type\"] == \"submission\"\n",
    "        total_submissions = self.df[type_mask].groupby(\"author\").size()\n",
    "        self.authors[\"total_submissions\"] = total_submissions\n",
    "        total_submission_score = self.df[type_mask].groupby(\"author\")[\"score\"].sum()\n",
    "        self.authors[\"total_submission_score\"] = total_submission_score\n",
    "        total_submission_comments = (\n",
    "            self.df[type_mask].groupby(\"author\")[\"num_comments\"].sum()\n",
    "        )\n",
    "        self.authors[\"total_submission_comments\"] = total_submission_comments\n",
    "        total_comments = self.df[~type_mask].groupby(\"author\").size()\n",
    "        self.authors[\"total_comments\"] = total_comments\n",
    "        total_comment_score = self.df[~type_mask].groupby(\"author\")[\"score\"].sum()\n",
    "        self.authors[\"total_comment_score\"] = total_comment_score\n",
    "        self.authors = self.authors.apply(lambda x: x.fillna(0), axis=1)\n",
    "        self.authors[\"total_posts\"] = (\n",
    "            self.authors[\"total_submissions\"] + self.authors[\"total_comments\"]\n",
    "        )\n",
    "        self.authors[\"total_post_score\"] = (\n",
    "            self.authors[\"total_submission_score\"] + self.authors[\"total_comment_score\"]\n",
    "        )\n",
    "        self.authors.index.name = \"author\"\n",
    "\n",
    "    def compare_authors(self, community):\n",
    "        \"\"\"\n",
    "        Perform outer and inner joins on the two community authors.\n",
    "        returns a tuple containing: (outer join DataFrame, inner join DataFrame)\n",
    "        \"\"\"\n",
    "        outer = pd.merge(\n",
    "            self.authors[[\"total_submissions\", \"total_comments\", \"total_posts\"]],\n",
    "            community.authors[[\"total_submissions\", \"total_comments\", \"total_posts\"]],\n",
    "            how=\"outer\",\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            suffixes=(\"_\" + self.name, \"_\" + community.name),\n",
    "        ).fillna(0)\n",
    "        inner = pd.merge(\n",
    "            self.authors[[\"total_submissions\", \"total_comments\", \"total_posts\"]],\n",
    "            community.authors[[\"total_submissions\", \"total_comments\", \"total_posts\"]],\n",
    "            how=\"inner\",\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            suffixes=(\"_\" + self.name, \"_\" + community.name),\n",
    "        ).fillna(0)\n",
    "        return outer, inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can take our queried DataFrames and create a Community object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conspiracy = community(name=\"conspiracy\", dataframe=conspiracy_query.df)\n",
    "antivax = community(name=\"antivax\", dataframe=antivax_query.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example finding urls embedded in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_type</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>datetime</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comment</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ghntqbx</td>\n",
       "      <td>t1_ghj2v5e</td>\n",
       "      <td>t3_kmr3wr</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/conspiracy/comments/kmr3wr/the_senate_finan...</td>\n",
       "      <td>1609463302</td>\n",
       "      <td>12/31/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Whataboutism</td>\n",
       "      <td>pinkerton--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>comment</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ghnupw4</td>\n",
       "      <td>t1_ghnp7h1</td>\n",
       "      <td>t3_knglp7</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/conspiracy/comments/knglp7/most_of_the_mate...</td>\n",
       "      <td>1609463895</td>\n",
       "      <td>12/31/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.reuters.com/article/uk-factcheck-h...</td>\n",
       "      <td>Jonisonice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>comment</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ghnvb41</td>\n",
       "      <td>t1_ghnv2j1</td>\n",
       "      <td>t3_ko289s</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/conspiracy/comments/ko289s/wuhan_china_news...</td>\n",
       "      <td>1609464248</td>\n",
       "      <td>12/31/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.google.com/amp/s/www.dailymail.co....</td>\n",
       "      <td>thatsgreatbruv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>comment</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ghnvf63</td>\n",
       "      <td>t1_ghnjf4j</td>\n",
       "      <td>t3_knu02c</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/conspiracy/comments/knu02c/audit_the_dead_c...</td>\n",
       "      <td>1609464317</td>\n",
       "      <td>12/31/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.usatoday.com/story/news/factcheck/...</td>\n",
       "      <td>Bond4141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>comment</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ghnwaui</td>\n",
       "      <td>t1_ghnp5ou</td>\n",
       "      <td>t3_kntlek</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/conspiracy/comments/kntlek/of_course_wherev...</td>\n",
       "      <td>1609464834</td>\n",
       "      <td>12/31/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.buzzfeednews.com/amphtml/rosiegray...</td>\n",
       "      <td>Annoyingly_Liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22403</th>\n",
       "      <td>comment</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ghv2jbd</td>\n",
       "      <td>t1_ghtyaq1</td>\n",
       "      <td>t3_kotrqb</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/conspiracy/comments/kotrqb/pence_was_on_af2...</td>\n",
       "      <td>1609621482</td>\n",
       "      <td>01/02/2021</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://archive.org/details/AntiCommunitarianM...</td>\n",
       "      <td>fraxurdfuture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>comment</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ghv3kr2</td>\n",
       "      <td>t1_ghuz60w</td>\n",
       "      <td>t3_kp3ypv</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/conspiracy/comments/kp3ypv/birds_lay_dead_i...</td>\n",
       "      <td>1609622027</td>\n",
       "      <td>01/02/2021</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.google.com/amp/s/www.forbes.com/si...</td>\n",
       "      <td>jamesko1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23007</th>\n",
       "      <td>comment</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ghv91o1</td>\n",
       "      <td>t1_ghv7v20</td>\n",
       "      <td>t3_kp5lxl</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/conspiracy/comments/kp5lxl/what_happens_whe...</td>\n",
       "      <td>1609624865</td>\n",
       "      <td>01/02/2021</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.google.com/amp/s/constitutioncente...</td>\n",
       "      <td>wileydickgoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23167</th>\n",
       "      <td>comment</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ghvar5a</td>\n",
       "      <td>t1_ghv5bm4</td>\n",
       "      <td>t3_kp19z7</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/conspiracy/comments/kp19z7/wheres_my_money_...</td>\n",
       "      <td>1609625757</td>\n",
       "      <td>01/02/2021</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://youtu.be/sY2Y-L5cvcA\\n\\nThere definite...</td>\n",
       "      <td>ZachElmurry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23208</th>\n",
       "      <td>comment</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>ghvb755</td>\n",
       "      <td>t1_ghvauy5</td>\n",
       "      <td>t3_klmy2z</td>\n",
       "      <td>nan</td>\n",
       "      <td>/r/conspiracy/comments/klmy2z/they_dont_care_a...</td>\n",
       "      <td>1609625986</td>\n",
       "      <td>01/02/2021</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>https://www.annualreviews.org/doi/pdf/10.1146/...</td>\n",
       "      <td>IosefkatheClinician</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_type   subreddit       id   parent_id    link_id  url  \\\n",
       "70      comment  conspiracy  ghntqbx  t1_ghj2v5e  t3_kmr3wr  nan   \n",
       "131     comment  conspiracy  ghnupw4  t1_ghnp7h1  t3_knglp7  nan   \n",
       "183     comment  conspiracy  ghnvb41  t1_ghnv2j1  t3_ko289s  nan   \n",
       "193     comment  conspiracy  ghnvf63  t1_ghnjf4j  t3_knu02c  nan   \n",
       "261     comment  conspiracy  ghnwaui  t1_ghnp5ou  t3_kntlek  nan   \n",
       "...         ...         ...      ...         ...        ...  ...   \n",
       "22403   comment  conspiracy  ghv2jbd  t1_ghtyaq1  t3_kotrqb  nan   \n",
       "22517   comment  conspiracy  ghv3kr2  t1_ghuz60w  t3_kp3ypv  nan   \n",
       "23007   comment  conspiracy  ghv91o1  t1_ghv7v20  t3_kp5lxl  nan   \n",
       "23167   comment  conspiracy  ghvar5a  t1_ghv5bm4  t3_kp19z7  nan   \n",
       "23208   comment  conspiracy  ghvb755  t1_ghvauy5  t3_klmy2z  nan   \n",
       "\n",
       "                                               permalink created_utc  \\\n",
       "70     /r/conspiracy/comments/kmr3wr/the_senate_finan...  1609463302   \n",
       "131    /r/conspiracy/comments/knglp7/most_of_the_mate...  1609463895   \n",
       "183    /r/conspiracy/comments/ko289s/wuhan_china_news...  1609464248   \n",
       "193    /r/conspiracy/comments/knu02c/audit_the_dead_c...  1609464317   \n",
       "261    /r/conspiracy/comments/kntlek/of_course_wherev...  1609464834   \n",
       "...                                                  ...         ...   \n",
       "22403  /r/conspiracy/comments/kotrqb/pence_was_on_af2...  1609621482   \n",
       "22517  /r/conspiracy/comments/kp3ypv/birds_lay_dead_i...  1609622027   \n",
       "23007  /r/conspiracy/comments/kp5lxl/what_happens_whe...  1609624865   \n",
       "23167  /r/conspiracy/comments/kp19z7/wheres_my_money_...  1609625757   \n",
       "23208  /r/conspiracy/comments/klmy2z/they_dont_care_a...  1609625986   \n",
       "\n",
       "         datetime score num_comments title  \\\n",
       "70     12/31/2020     1          nan   nan   \n",
       "131    12/31/2020     1          nan   nan   \n",
       "183    12/31/2020     1          nan   nan   \n",
       "193    12/31/2020     1          nan   nan   \n",
       "261    12/31/2020     1          nan   nan   \n",
       "...           ...   ...          ...   ...   \n",
       "22403  01/02/2021     1          nan   nan   \n",
       "22517  01/02/2021     1          nan   nan   \n",
       "23007  01/02/2021     1          nan   nan   \n",
       "23167  01/02/2021     1          nan   nan   \n",
       "23208  01/02/2021     1          nan   nan   \n",
       "\n",
       "                                                    body               author  \n",
       "70            https://en.wikipedia.org/wiki/Whataboutism          pinkerton--  \n",
       "131    https://www.reuters.com/article/uk-factcheck-h...           Jonisonice  \n",
       "183    https://www.google.com/amp/s/www.dailymail.co....       thatsgreatbruv  \n",
       "193    https://www.usatoday.com/story/news/factcheck/...             Bond4141  \n",
       "261    https://www.buzzfeednews.com/amphtml/rosiegray...   Annoyingly_Liberal  \n",
       "...                                                  ...                  ...  \n",
       "22403  https://archive.org/details/AntiCommunitarianM...        fraxurdfuture  \n",
       "22517  https://www.google.com/amp/s/www.forbes.com/si...          jamesko1989  \n",
       "23007  https://www.google.com/amp/s/constitutioncente...         wileydickgoo  \n",
       "23167  https://youtu.be/sY2Y-L5cvcA\\n\\nThere definite...          ZachElmurry  \n",
       "23208  https://www.annualreviews.org/doi/pdf/10.1146/...  IosefkatheClinician  \n",
       "\n",
       "[162 rows x 14 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conspiracy.make_urls(column=\"body\", post_type=\"comment\")\n",
    "conspiracy.url_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our community will store the urls after making them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Whataboutism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>https://www.reuters.com/article/uk-factcheck-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>https://www.google.com/amp/s/www.dailymail.co....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>https://www.usatoday.com/story/news/factcheck/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>https://www.buzzfeednews.com/amphtml/rosiegray...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22403</th>\n",
       "      <td>https://archive.org/details/AntiCommunitarianM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>https://www.google.com/amp/s/www.forbes.com/si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23007</th>\n",
       "      <td>https://www.google.com/amp/s/constitutioncente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23167</th>\n",
       "      <td>https://youtu.be/sY2Y-L5cvcA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23208</th>\n",
       "      <td>https://www.annualreviews.org/doi/pdf/10.1146/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url\n",
       "70            https://en.wikipedia.org/wiki/Whataboutism\n",
       "131    https://www.reuters.com/article/uk-factcheck-h...\n",
       "183    https://www.google.com/amp/s/www.dailymail.co....\n",
       "193    https://www.usatoday.com/story/news/factcheck/...\n",
       "261    https://www.buzzfeednews.com/amphtml/rosiegray...\n",
       "...                                                  ...\n",
       "22403  https://archive.org/details/AntiCommunitarianM...\n",
       "22517  https://www.google.com/amp/s/www.forbes.com/si...\n",
       "23007  https://www.google.com/amp/s/constitutioncente...\n",
       "23167                       https://youtu.be/sY2Y-L5cvcA\n",
       "23208  https://www.annualreviews.org/doi/pdf/10.1146/...\n",
       "\n",
       "[162 rows x 1 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conspiracy.urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also qurey our community on authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_submissions</th>\n",
       "      <th>total_submission_score</th>\n",
       "      <th>total_submission_comments</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>total_comment_score</th>\n",
       "      <th>total_posts</th>\n",
       "      <th>total_post_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bmille3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yotta_e</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vegannibba</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hiasfukit</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WheniamHigh</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lupusvorax</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ghost_of_mr_chicken</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blurtard</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bocasdt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ninjatoes36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     total_submissions  total_submission_score  \\\n",
       "author                                                           \n",
       "Bmille3                            2.0                     2.0   \n",
       "yotta_e                            1.0                     1.0   \n",
       "Vegannibba                         2.0                     2.0   \n",
       "hiasfukit                          1.0                     1.0   \n",
       "WheniamHigh                        2.0                     2.0   \n",
       "...                                ...                     ...   \n",
       "Lupusvorax                         0.0                     0.0   \n",
       "ghost_of_mr_chicken                0.0                     0.0   \n",
       "blurtard                           0.0                     0.0   \n",
       "bocasdt                            0.0                     0.0   \n",
       "ninjatoes36                        0.0                     0.0   \n",
       "\n",
       "                     total_submission_comments  total_comments  \\\n",
       "author                                                           \n",
       "Bmille3                                   86.0            13.0   \n",
       "yotta_e                                   37.0            10.0   \n",
       "Vegannibba                                47.0             3.0   \n",
       "hiasfukit                                  2.0             2.0   \n",
       "WheniamHigh                                6.0             1.0   \n",
       "...                                        ...             ...   \n",
       "Lupusvorax                                 0.0             3.0   \n",
       "ghost_of_mr_chicken                        0.0             1.0   \n",
       "blurtard                                   0.0             1.0   \n",
       "bocasdt                                    0.0             1.0   \n",
       "ninjatoes36                                0.0             1.0   \n",
       "\n",
       "                     total_comment_score  total_posts  total_post_score  \n",
       "author                                                                   \n",
       "Bmille3                             13.0         15.0              15.0  \n",
       "yotta_e                             10.0         11.0              11.0  \n",
       "Vegannibba                           3.0          5.0               5.0  \n",
       "hiasfukit                            2.0          3.0               3.0  \n",
       "WheniamHigh                          1.0          3.0               3.0  \n",
       "...                                  ...          ...               ...  \n",
       "Lupusvorax                           3.0          3.0               3.0  \n",
       "ghost_of_mr_chicken                  1.0          1.0               1.0  \n",
       "blurtard                             1.0          1.0               1.0  \n",
       "bocasdt                              1.0          1.0               1.0  \n",
       "ninjatoes36                          1.0          1.0               1.0  \n",
       "\n",
       "[8200 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conspiracy.make_authors()\n",
    "antivax.make_authors()\n",
    "conspiracy.authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given 2 Communities we can make pairwise comparisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_submissions_conspiracy</th>\n",
       "      <th>total_comments_conspiracy</th>\n",
       "      <th>total_posts_conspiracy</th>\n",
       "      <th>total_submissions_antivax</th>\n",
       "      <th>total_comments_antivax</th>\n",
       "      <th>total_posts_antivax</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>---Seraphim---</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--Gem</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--dontmindme--</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-5x-</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-80watt-</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoso418</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuzuofthewolves</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zx12y</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzjient</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzzzz_</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8238 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 total_submissions_conspiracy  total_comments_conspiracy  \\\n",
       "author                                                                     \n",
       "---Seraphim---                            0.0                        1.0   \n",
       "--Gem                                     0.0                        1.0   \n",
       "--dontmindme--                            0.0                        2.0   \n",
       "-5x-                                      0.0                        2.0   \n",
       "-80watt-                                  0.0                        1.0   \n",
       "...                                       ...                        ...   \n",
       "zoso418                                   1.0                        0.0   \n",
       "zuzuofthewolves                           0.0                       14.0   \n",
       "zx12y                                     0.0                        1.0   \n",
       "zzjient                                   3.0                        0.0   \n",
       "zzzzz_                                    0.0                        1.0   \n",
       "\n",
       "                 total_posts_conspiracy  total_submissions_antivax  \\\n",
       "author                                                               \n",
       "---Seraphim---                      1.0                        0.0   \n",
       "--Gem                               1.0                        0.0   \n",
       "--dontmindme--                      2.0                        0.0   \n",
       "-5x-                                2.0                        0.0   \n",
       "-80watt-                            1.0                        0.0   \n",
       "...                                 ...                        ...   \n",
       "zoso418                             1.0                        0.0   \n",
       "zuzuofthewolves                    14.0                        0.0   \n",
       "zx12y                               1.0                        0.0   \n",
       "zzjient                             3.0                        0.0   \n",
       "zzzzz_                              1.0                        0.0   \n",
       "\n",
       "                 total_comments_antivax  total_posts_antivax  \n",
       "author                                                        \n",
       "---Seraphim---                      0.0                  0.0  \n",
       "--Gem                               0.0                  0.0  \n",
       "--dontmindme--                      0.0                  0.0  \n",
       "-5x-                                0.0                  0.0  \n",
       "-80watt-                            0.0                  0.0  \n",
       "...                                 ...                  ...  \n",
       "zoso418                             0.0                  0.0  \n",
       "zuzuofthewolves                     0.0                  0.0  \n",
       "zx12y                               0.0                  0.0  \n",
       "zzjient                             0.0                  0.0  \n",
       "zzzzz_                              0.0                  0.0  \n",
       "\n",
       "[8238 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outer, inner = conspiracy.compare_authors(antivax)\n",
    "outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_submissions_conspiracy    0.002896\n",
       "total_comments_conspiracy       0.054312\n",
       "total_posts_conspiracy          0.052711\n",
       "total_submissions_antivax       0.000000\n",
       "total_comments_antivax          0.101266\n",
       "total_posts_antivax             0.096386\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inner.sum() / outer.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create subreddit reference object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class subreddits:\n",
    "    \"\"\"\n",
    "    Class for making queries based on subreddit type.\n",
    "    The resulting DataFrame is meant to be used as a selection matrix based on subreddit.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path=None, file_format=None):\n",
    "        \"\"\"\n",
    "        Initilization of object, reads in subreddit list and adds datetime column.\n",
    "        ----------\n",
    "        paramaters\n",
    "        ----------\n",
    "        path: location of subredded statistics.\n",
    "        file_format: formate to be specified if path is given.\n",
    "            \"csv\"- if provided path points to .csv file.\n",
    "            \"pkl\"- if provided path points to pickled DataFrame.\n",
    "        \"\"\"\n",
    "        if path == None:\n",
    "            self.master = pd.read_csv(\n",
    "                \"F:\\Research\\Funded\\Ethical_Reccomendations\\Python\\Data\\Docs\\subreddit_list.csv\"\n",
    "            )\n",
    "        else:\n",
    "            if file_format == \"csv\":\n",
    "                self.master = pd.read_csv(path)\n",
    "            elif file_format == \"pkl\":\n",
    "                self.master = pd.read_pickle(path)\n",
    "        self.master[\"Creation_DateTime\"] = [\n",
    "            datetime.datetime.fromtimestamp(int(utc))\n",
    "            for utc in self.master[\"Creation_UTC\"]\n",
    "        ]\n",
    "\n",
    "    def split_nsfw(self):\n",
    "        \"\"\"\n",
    "        Create attributes containig masked DataFrames for Not Safe / Safe For Work subreddits.\n",
    "        \"\"\"\n",
    "        nsfw_mask = self.master[\"NSFW_BOOL\"] == True\n",
    "        self.nsfw = self.master[nsfw_mask]\n",
    "        self.sfw = self.master[~nsfw_mask]\n",
    "\n",
    "    def split_size(self, min_subscribers=0, max_subscribers=9999999999):\n",
    "        \"\"\"\n",
    "        Create attribute containing subreddits within a specified subscriber range.\n",
    "        ----------\n",
    "        paramaters\n",
    "        ----------\n",
    "        min_subscribers: minimum number of allowed subscribers to a subreddit.\n",
    "        max_subscribers: maximum number of allowed subscribers to a subreddit.\n",
    "        \"\"\"\n",
    "        min_size_mask = self.master[\"#_Subscribers\"] >= min_subscribers\n",
    "        max_size_mask = self.master[\"#_Subscribers\"] <= max_subscribers\n",
    "        self.sized = self.master[min_size_mask & max_size_mask]\n",
    "\n",
    "    def split_creation_time_unix(\n",
    "        self, min_unix_timestamp=0000000000, max_unix_timestamp=9999999999\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create attribute containing subreddits created within a specified time range.\n",
    "        ----------\n",
    "        paramaters\n",
    "        ----------\n",
    "        min_unix_timestamp: earliest allowed date of creation in unix ephoch timestamp.\n",
    "        max_unix_timestamp: latest allowed date of creation in unix ephoch timestamp.\n",
    "        \"\"\"\n",
    "        min_unix_time_mask = self.master[\"Creation_UTC\"] >= min_unix_timestamp\n",
    "        max_unix_time_mask = self.master[\"Creation_UTC\"] <= max_unix_timestamp\n",
    "        self.sized = self.master[min_unix_time_mask & max_unix_time_mask]\n",
    "\n",
    "    def split_creation_time_date(\n",
    "        self, min_datetime=\"2000-01-01\", max_datetime=\"2022-02-02\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create attribute containing subreddits created within a specified time range.\n",
    "        ----------\n",
    "        paramaters\n",
    "        ----------\n",
    "        min_datetime: earliest allowed date of creation in datetime format.\n",
    "        max_datetime: latest allowed date of creation in datetime format.\n",
    "        \"\"\"\n",
    "        min_date_time_mask = self.master[\"Creation_DateTime\"] >= min_datetime\n",
    "        max_date_time_mask = self.master[\"Creation_DateTime\"] <= max_datetime\n",
    "        self.sized = self.master[min_date_time_mask & max_date_time_mask]\n",
    "\n",
    "    def split_multi(self, nsfw=None, sizes=None, unix_times=None, date_times=None):\n",
    "        \"\"\"\n",
    "        Query based subreddit selction based on NSFW status, subscriber count, and creation time.\n",
    "        ----------\n",
    "        paramaters\n",
    "        ----------\n",
    "        nsfw: True or False\n",
    "        sizes: dictionary input {'min_subscribers' : minimum_value, 'max_subscribers' : maximum_value}\n",
    "        unix_times: dictionary input {'min_unix_timestamp' : minimum_timestamp, 'max_unix_timestamp' : maximum_timestamp}\n",
    "        date_times: dictionary input {'min_datetime' : 'minimum_datetime', 'max_datetime' : 'maximum_datetime}\n",
    "        \"\"\"\n",
    "        if nsfw == None:\n",
    "            nsfw_mask = [True for _ in self.master.index]\n",
    "        else:\n",
    "            if nsfw == True:\n",
    "                nsfw_mask = self.master[\"NSFW_BOOL\"] == True\n",
    "            elif nsfw == False:\n",
    "                nsfw_mask = self.master[\"NSFW_BOOL\"] == False\n",
    "        if sizes == None:\n",
    "            min_size_mask = [True for _ in self.master.index]\n",
    "            max_size_mask = [True for _ in self.master.index]\n",
    "        else:\n",
    "            min_size_mask = self.master[\"#_Subscribers\"] >= sizes[\"min_subscribers\"]\n",
    "            max_size_mask = self.master[\"#_Subscribers\"] <= sizes[\"max_subscribers\"]\n",
    "        if unix_times == None:\n",
    "            min_unix_time_mask = [True for _ in self.master.index]\n",
    "            max_unix_time_mask = [True for _ in self.master.index]\n",
    "        else:\n",
    "            min_unix_time_mask = (\n",
    "                self.master[\"Creation_UTC\"] >= unix_times[\"min_unix_timestamp\"]\n",
    "            )\n",
    "            max_unix_time_mask = (\n",
    "                self.master[\"Creation_UTC\"] <= unix_times[\"max_unix_timestamp\"]\n",
    "            )\n",
    "        if date_times == None:\n",
    "            min_date_time_mask = [True for _ in self.master.index]\n",
    "            max_date_time_mask = [True for _ in self.master.index]\n",
    "        else:\n",
    "            min_date_time_mask = (\n",
    "                self.master[\"Creation_DateTime\"] >= date_times[\"min_datetime\"]\n",
    "            )\n",
    "            max_date_time_mask = (\n",
    "                self.master[\"Creation_DateTime\"] <= date_times[\"max_datetime\"]\n",
    "            )\n",
    "\n",
    "        self.multi = self.master[\n",
    "            nsfw_mask\n",
    "            & min_size_mask\n",
    "            & max_size_mask\n",
    "            & min_unix_time_mask\n",
    "            & max_unix_time_mask\n",
    "            & min_date_time_mask\n",
    "            & max_date_time_mask\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can run queries to search for subreddits by condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>#_Subscribers</th>\n",
       "      <th>Creation_UTC</th>\n",
       "      <th>NSFW_BOOL</th>\n",
       "      <th>Creation_DateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>Wallstreetbetsnew</td>\n",
       "      <td>830203</td>\n",
       "      <td>1.584312e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-15 18:34:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>Crypto traders with diamond hands 💎🙌</td>\n",
       "      <td>507399</td>\n",
       "      <td>1.582466e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-23 09:00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>byebyejob</td>\n",
       "      <td>493627</td>\n",
       "      <td>1.591489e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-06-06 20:20:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>SHIBArmy</td>\n",
       "      <td>444642</td>\n",
       "      <td>1.596479e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-08-03 14:27:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>WallStreetbetsELITE</td>\n",
       "      <td>430701</td>\n",
       "      <td>1.584912e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-22 17:15:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>Awarded… posthumously.</td>\n",
       "      <td>418724</td>\n",
       "      <td>1.600648e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-20 20:24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>r/GoodAnimemes</td>\n",
       "      <td>402648</td>\n",
       "      <td>1.596521e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-08-04 01:59:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>353131</td>\n",
       "      <td>1.581434e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11 10:16:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>Distant Socializing</td>\n",
       "      <td>345052</td>\n",
       "      <td>1.584321e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-15 21:02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>Anime Titties</td>\n",
       "      <td>308258</td>\n",
       "      <td>1.588832e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-05-07 02:13:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>skamtebord</td>\n",
       "      <td>235739</td>\n",
       "      <td>1.582500e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-23 18:24:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>Genshin_Memepact</td>\n",
       "      <td>225199</td>\n",
       "      <td>1.601918e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-10-05 13:05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>Animal Crossing New Horizons</td>\n",
       "      <td>199756</td>\n",
       "      <td>1.581268e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-09 12:07:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>TrashTaste</td>\n",
       "      <td>189341</td>\n",
       "      <td>1.590668e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-05-28 08:16:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>dreamsmp</td>\n",
       "      <td>186945</td>\n",
       "      <td>1.594565e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-12 10:46:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>SPACs</td>\n",
       "      <td>183299</td>\n",
       "      <td>1.589996e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-05-20 13:40:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>r/fakedisordercringe</td>\n",
       "      <td>180274</td>\n",
       "      <td>1.597945e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-08-20 13:35:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>JustGuysBeingDudes</td>\n",
       "      <td>178155</td>\n",
       "      <td>1.599793e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-10 22:50:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>FloridaCoronavirus</td>\n",
       "      <td>175560</td>\n",
       "      <td>1.583558e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-07 00:16:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>MLA_Official</td>\n",
       "      <td>167130</td>\n",
       "      <td>1.595231e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-20 03:44:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>ParlerWatch</td>\n",
       "      <td>163120</td>\n",
       "      <td>1.604970e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-11-09 20:07:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>Reddit Master Classes</td>\n",
       "      <td>158645</td>\n",
       "      <td>1.582062e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-18 16:42:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>Genshin Impact Leaks</td>\n",
       "      <td>157111</td>\n",
       "      <td>1.608345e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-18 21:25:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3517</th>\n",
       "      <td>Chadtopia</td>\n",
       "      <td>156875</td>\n",
       "      <td>1.580783e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-03 21:24:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>Police brutality in the United States during t...</td>\n",
       "      <td>156803</td>\n",
       "      <td>1.590941e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-05-31 12:03:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>Onlyfans creator's community--&amp;gt; Advice, dis...</td>\n",
       "      <td>155032</td>\n",
       "      <td>1.588251e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-04-30 08:48:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>CoronavirusUS</td>\n",
       "      <td>144701</td>\n",
       "      <td>1.581535e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-12 14:12:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>Pokémon UNITE</td>\n",
       "      <td>143903</td>\n",
       "      <td>1.593004e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-06-24 09:03:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3830</th>\n",
       "      <td>CoronavirusDownunder</td>\n",
       "      <td>142569</td>\n",
       "      <td>1.582419e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-22 19:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3897</th>\n",
       "      <td>NarcoFootage</td>\n",
       "      <td>139904</td>\n",
       "      <td>1.582941e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-28 20:48:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011</th>\n",
       "      <td>VALORANT Competitive</td>\n",
       "      <td>135420</td>\n",
       "      <td>1.582477e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-23 12:04:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>LowSodiumCyberpunk</td>\n",
       "      <td>135006</td>\n",
       "      <td>1.607616e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-10 10:53:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4033</th>\n",
       "      <td>StreetMartialArts</td>\n",
       "      <td>134733</td>\n",
       "      <td>1.585671e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-31 12:04:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4061</th>\n",
       "      <td>CoronavirusWA</td>\n",
       "      <td>133540</td>\n",
       "      <td>1.583102e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-01 17:38:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4165</th>\n",
       "      <td>📚 Read With Me 📖</td>\n",
       "      <td>130075</td>\n",
       "      <td>1.591316e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-06-04 20:17:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4216</th>\n",
       "      <td>coronanetherlands</td>\n",
       "      <td>128127</td>\n",
       "      <td>1.584049e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-12 17:39:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>Coronavírus Brasil (COVID-19)</td>\n",
       "      <td>127765</td>\n",
       "      <td>1.584281e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-15 10:06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>/r/COVID-19Positive</td>\n",
       "      <td>124913</td>\n",
       "      <td>1.584147e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-13 20:56:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>Idiocy is contagious.</td>\n",
       "      <td>123734</td>\n",
       "      <td>1.584494e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-17 21:10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>GhanaSaysGoodbye</td>\n",
       "      <td>123698</td>\n",
       "      <td>1.585615e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-30 20:44:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>CoronavirusIllinois</td>\n",
       "      <td>122165</td>\n",
       "      <td>1.583293e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-03 22:43:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4418</th>\n",
       "      <td>DylanteroYT</td>\n",
       "      <td>121296</td>\n",
       "      <td>1.591385e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-06-05 15:27:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>COVID-19 Pandemic in the Philippines</td>\n",
       "      <td>117656</td>\n",
       "      <td>1.583500e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-06 08:09:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4549</th>\n",
       "      <td>NonPoliticalTwitter</td>\n",
       "      <td>117013</td>\n",
       "      <td>1.602801e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-10-15 18:27:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>NoFeeAC</td>\n",
       "      <td>114648</td>\n",
       "      <td>1.587417e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-04-20 17:07:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659</th>\n",
       "      <td>Tiger King: Murder, Mayhem and Madness</td>\n",
       "      <td>113833</td>\n",
       "      <td>1.582082e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-18 22:09:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4697</th>\n",
       "      <td>Irrational Madness</td>\n",
       "      <td>112446</td>\n",
       "      <td>1.590397e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-05-25 04:50:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>Animal Crossing Design</td>\n",
       "      <td>108845</td>\n",
       "      <td>1.584921e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-22 19:48:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4903</th>\n",
       "      <td>Friday Night Funkin' - Rhythm Gaming Excellence</td>\n",
       "      <td>107326</td>\n",
       "      <td>1.602283e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-10-09 18:37:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4913</th>\n",
       "      <td>Looney Tunes Logic</td>\n",
       "      <td>107125</td>\n",
       "      <td>1.590874e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-05-30 17:20:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>Im too balkan for your standard</td>\n",
       "      <td>106761</td>\n",
       "      <td>1.590750e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-05-29 07:07:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4942</th>\n",
       "      <td>keoXer</td>\n",
       "      <td>106608</td>\n",
       "      <td>1.596571e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-08-04 16:02:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>CoronavirusCA</td>\n",
       "      <td>104928</td>\n",
       "      <td>1.583104e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-03-01 18:06:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5133</th>\n",
       "      <td>HUEstation</td>\n",
       "      <td>101915</td>\n",
       "      <td>1.581992e+09</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-17 21:18:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Subreddit  #_Subscribers  \\\n",
       "650                                   Wallstreetbetsnew         830203   \n",
       "1066               Crypto traders with diamond hands 💎🙌         507399   \n",
       "1094                                          byebyejob         493627   \n",
       "1224                                           SHIBArmy         444642   \n",
       "1268                                WallStreetbetsELITE         430701   \n",
       "1309                             Awarded… posthumously.         418724   \n",
       "1351                                     r/GoodAnimemes         402648   \n",
       "1560                                           COVID-19         353131   \n",
       "1601                                Distant Socializing         345052   \n",
       "1810                                      Anime Titties         308258   \n",
       "2346                                         skamtebord         235739   \n",
       "2464                                   Genshin_Memepact         225199   \n",
       "2765                       Animal Crossing New Horizons         199756   \n",
       "2909                                         TrashTaste         189341   \n",
       "2951                                           dreamsmp         186945   \n",
       "3016                                              SPACs         183299   \n",
       "3067                               r/fakedisordercringe         180274   \n",
       "3111                                 JustGuysBeingDudes         178155   \n",
       "3163                                 FloridaCoronavirus         175560   \n",
       "3320                                       MLA_Official         167130   \n",
       "3400                                        ParlerWatch         163120   \n",
       "3472                              Reddit Master Classes         158645   \n",
       "3510                               Genshin Impact Leaks         157111   \n",
       "3517                                          Chadtopia         156875   \n",
       "3519  Police brutality in the United States during t...         156803   \n",
       "3547  Onlyfans creator's community--&gt; Advice, dis...         155032   \n",
       "3777                                      CoronavirusUS         144701   \n",
       "3799                                      Pokémon UNITE         143903   \n",
       "3830                               CoronavirusDownunder         142569   \n",
       "3897                                       NarcoFootage         139904   \n",
       "4011                               VALORANT Competitive         135420   \n",
       "4021                                 LowSodiumCyberpunk         135006   \n",
       "4033                                  StreetMartialArts         134733   \n",
       "4061                                      CoronavirusWA         133540   \n",
       "4165                                   📚 Read With Me 📖         130075   \n",
       "4216                                  coronanetherlands         128127   \n",
       "4232                      Coronavírus Brasil (COVID-19)         127765   \n",
       "4314                                /r/COVID-19Positive         124913   \n",
       "4348                              Idiocy is contagious.         123734   \n",
       "4349                                   GhanaSaysGoodbye         123698   \n",
       "4387                                CoronavirusIllinois         122165   \n",
       "4418                                        DylanteroYT         121296   \n",
       "4528               COVID-19 Pandemic in the Philippines         117656   \n",
       "4549                                NonPoliticalTwitter         117013   \n",
       "4630                                            NoFeeAC         114648   \n",
       "4659            Tiger King: Murder, Mayhem and Madness          113833   \n",
       "4697                                 Irrational Madness         112446   \n",
       "4839                             Animal Crossing Design         108845   \n",
       "4903    Friday Night Funkin' - Rhythm Gaming Excellence         107326   \n",
       "4913                                 Looney Tunes Logic         107125   \n",
       "4933                    Im too balkan for your standard         106761   \n",
       "4942                                             keoXer         106608   \n",
       "5010                                      CoronavirusCA         104928   \n",
       "5133                                         HUEstation         101915   \n",
       "\n",
       "      Creation_UTC  NSFW_BOOL   Creation_DateTime  \n",
       "650   1.584312e+09      False 2020-03-15 18:34:59  \n",
       "1066  1.582466e+09      False 2020-02-23 09:00:27  \n",
       "1094  1.591489e+09      False 2020-06-06 20:20:38  \n",
       "1224  1.596479e+09      False 2020-08-03 14:27:30  \n",
       "1268  1.584912e+09      False 2020-03-22 17:15:30  \n",
       "1309  1.600648e+09      False 2020-09-20 20:24:02  \n",
       "1351  1.596521e+09      False 2020-08-04 01:59:49  \n",
       "1560  1.581434e+09      False 2020-02-11 10:16:21  \n",
       "1601  1.584321e+09      False 2020-03-15 21:02:18  \n",
       "1810  1.588832e+09      False 2020-05-07 02:13:07  \n",
       "2346  1.582500e+09      False 2020-02-23 18:24:19  \n",
       "2464  1.601918e+09      False 2020-10-05 13:05:30  \n",
       "2765  1.581268e+09      False 2020-02-09 12:07:21  \n",
       "2909  1.590668e+09      False 2020-05-28 08:16:58  \n",
       "2951  1.594565e+09      False 2020-07-12 10:46:47  \n",
       "3016  1.589996e+09      False 2020-05-20 13:40:20  \n",
       "3067  1.597945e+09      False 2020-08-20 13:35:20  \n",
       "3111  1.599793e+09      False 2020-09-10 22:50:18  \n",
       "3163  1.583558e+09      False 2020-03-07 00:16:04  \n",
       "3320  1.595231e+09      False 2020-07-20 03:44:24  \n",
       "3400  1.604970e+09      False 2020-11-09 20:07:21  \n",
       "3472  1.582062e+09      False 2020-02-18 16:42:42  \n",
       "3510  1.608345e+09      False 2020-12-18 21:25:40  \n",
       "3517  1.580783e+09      False 2020-02-03 21:24:10  \n",
       "3519  1.590941e+09      False 2020-05-31 12:03:09  \n",
       "3547  1.588251e+09      False 2020-04-30 08:48:12  \n",
       "3777  1.581535e+09      False 2020-02-12 14:12:13  \n",
       "3799  1.593004e+09      False 2020-06-24 09:03:41  \n",
       "3830  1.582419e+09      False 2020-02-22 19:43:00  \n",
       "3897  1.582941e+09      False 2020-02-28 20:48:03  \n",
       "4011  1.582477e+09      False 2020-02-23 12:04:29  \n",
       "4021  1.607616e+09      False 2020-12-10 10:53:06  \n",
       "4033  1.585671e+09      False 2020-03-31 12:04:32  \n",
       "4061  1.583102e+09      False 2020-03-01 17:38:20  \n",
       "4165  1.591316e+09      False 2020-06-04 20:17:05  \n",
       "4216  1.584049e+09      False 2020-03-12 17:39:51  \n",
       "4232  1.584281e+09      False 2020-03-15 10:06:29  \n",
       "4314  1.584147e+09      False 2020-03-13 20:56:35  \n",
       "4348  1.584494e+09      False 2020-03-17 21:10:33  \n",
       "4349  1.585615e+09      False 2020-03-30 20:44:26  \n",
       "4387  1.583293e+09      False 2020-03-03 22:43:33  \n",
       "4418  1.591385e+09      False 2020-06-05 15:27:08  \n",
       "4528  1.583500e+09      False 2020-03-06 08:09:26  \n",
       "4549  1.602801e+09      False 2020-10-15 18:27:05  \n",
       "4630  1.587417e+09      False 2020-04-20 17:07:23  \n",
       "4659  1.582082e+09      False 2020-02-18 22:09:13  \n",
       "4697  1.590397e+09      False 2020-05-25 04:50:36  \n",
       "4839  1.584921e+09      False 2020-03-22 19:48:01  \n",
       "4903  1.602283e+09      False 2020-10-09 18:37:49  \n",
       "4913  1.590874e+09      False 2020-05-30 17:20:51  \n",
       "4933  1.590750e+09      False 2020-05-29 07:07:27  \n",
       "4942  1.596571e+09      False 2020-08-04 16:02:36  \n",
       "5010  1.583104e+09      False 2020-03-01 18:06:40  \n",
       "5133  1.581992e+09      False 2020-02-17 21:18:41  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_info = subreddits()\n",
    "size = {\"min_subscribers\": 100000, \"max_subscribers\": 9999999999}\n",
    "# unix_time = {\"min_unix_timestamp\": 1111111111, \"max_unix_timestamp\": 9999999999}\n",
    "date_time = {\"min_datetime\": \"2020-02-02\", \"max_datetime\": \"2021-01-01\"}\n",
    "subreddit_info.split_multi(nsfw=False, sizes=size, date_times=date_time)\n",
    "subreddit_info.multi"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
